{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Video Pre-processing:\n",
    "    - Extract Frames:\n",
    "        - Use a video processing tool like OpenCV to extract frames from the .avi video files at a consistent frame rate (e.g., 30 FPS).\n",
    "        Synchronize frame extraction with the timestamps provided in the time labels (from the utterance summary in the evaluation folder). This ensures each utterance is processed with the correct time alignment.\n",
    "    - Tools: OpenCV (Python), FFmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ses01F_impro02.avi', 'Ses01M_impro06.avi', '.DS_Store', 'Ses01F_script01_2.avi', 'Ses01M_impro03.avi', 'Ses01M_script02_1.avi', 'Ses01F_impro01.avi', 'Ses01F_impro03.avi', 'Ses01F_script02_1.avi', 'Ses01M_script01_1.avi', 'Thumbs.db', 'Ses01M_script03_1.avi', 'Ses01F_script01_3.avi', 'Ses01F_impro07.avi', 'Ses01F_script01_1.avi', 'Ses01F_impro05.avi', 'Ses01M_impro04.avi', 'Ses01F_impro04.avi', 'Ses01M_script01_3.avi', 'Ses01M_script03_2.avi', 'Ses01M_script02_2.avi', 'Ses01M_impro05.avi', 'Ses01M_impro07.avi', 'Ses01F_script03_1.avi', 'Ses01F_script02_2.avi', 'Ses01F_script03_2.avi', 'Ses01M_impro02.avi', 'Ses01M_script01_2.avi', 'Ses01F_impro06.avi', 'Ses01M_impro01.avi']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def list_files_in_folder(folder_path):\n",
    "    # Get all items in the folder\n",
    "    items = os.listdir(folder_path)\n",
    "\n",
    "    # Filter out directories, keeping only files\n",
    "    files = [item for item in items if os.path.isfile(os.path.join(folder_path, item))]\n",
    "\n",
    "    return files\n",
    "\n",
    "# Example usage\n",
    "folder_path = '../data/raw/iemocap/IEMOCAP_full_release/Session1/dialog/avi/DivX/'\n",
    "files = list_files_in_folder(folder_path)\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [06:24<00:00, 12.41s/it]\n",
      "100%|██████████| 32/32 [07:56<00:00, 14.89s/it]\n",
      "100%|██████████| 31/31 [07:03<00:00, 13.65s/it]\n",
      "100%|██████████| 32/32 [07:04<00:00, 13.26s/it]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_video_segments(video_path, lab_file, output_folder):\n",
    "    # Open video file\n",
    "    if not os.path.exists(video_path):\n",
    "        raise FileNotFoundError(f\"Video file not found: {video_path}\")\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Read utterance timing from lab file\n",
    "    with open(lab_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        # Parse start and end time, and utterance name\n",
    "        start_time, end_time, utterance_name = line.split()[:3]\n",
    "        start_frame = int(float(start_time) * fps)\n",
    "        end_frame = int(float(end_time) * fps)\n",
    "\n",
    "        # Set video to start frame\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "        # Read and save frames for the utterance\n",
    "        frames = []\n",
    "        for _ in range(start_frame, end_frame + 1):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frames.append(frame)\n",
    "\n",
    "        if not frames:\n",
    "            raise ValueError(f\"No frames extracted for utterance {utterance_name}\")\n",
    "\n",
    "        # Save the extracted frames as a video clip\n",
    "        out = cv2.VideoWriter(f'{output_folder}/{utterance_name}.avi', \n",
    "                              cv2.VideoWriter_fourcc(*'XVID'), fps, (frames[0].shape[1], frames[0].shape[0]))\n",
    "        for frame in frames:\n",
    "            out.write(frame)\n",
    "\n",
    "        out.release()\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "def main():\n",
    "    avi_dir_list = [\n",
    "        f\"../data/raw/iemocap/IEMOCAP_full_release/Session{i}/dialog/avi/DivX/\" for i in range(2, 6)\n",
    "    ]\n",
    "    lab_dir_list = [\n",
    "        f\"../data/raw/iemocap/IEMOCAP_full_release/Session{i}/dialog/lab/Ses0{i}_F/\" for i in range(2, 6)\n",
    "    ]\n",
    "\n",
    "    for i, directory_tuple in enumerate(zip(avi_dir_list, lab_dir_list)):\n",
    "        video_dir, lab_dir = directory_tuple\n",
    "        files = list_files_in_folder(video_dir)\n",
    "        for file in tqdm(files):\n",
    "            if file.split('.')[-1] != 'avi':\n",
    "                continue\n",
    "            video_path = video_dir + file\n",
    "            lab_file = lab_dir + f\"{file.split('.')[0]}.lab\"\n",
    "            output_folder = f\"../data/interim/iemocap/Session{i + 2}\"\n",
    "            os.makedirs(output_folder, exist_ok=True)\n",
    "            try:\n",
    "                extract_video_segments(video_path, lab_file, output_folder)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {e}\")\n",
    "                print(f\"Error occured at {video_path}, {lab_file}\")\n",
    "                break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Face Detection and Alignment\n",
    "    - Detect Face and Facial Landmarks:\n",
    "        - For each frame, apply a facial detection model (e.g., Dlib or OpenCV's face detector) to locate the face.\n",
    "        Use facial landmark detectors to identify key facial features (e.g., eyes, mouth, nose, jawline).\n",
    "        Align the face to a canonical position (rotated or scaled so that the eyes are horizontally aligned) for consistent feature extraction.\n",
    "    - Tools: Dlib, OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "from imutils import face_utils\n",
    "\n",
    "# Load pre-trained face detector and shape predictor models\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "def detect_landmarks(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "    landmarks_list = []\n",
    "    for face in faces:\n",
    "        shape = predictor(gray, face)\n",
    "        landmarks = face_utils.shape_to_np(shape)  # Convert to (x, y) coordinates\n",
    "        landmarks_list.append(landmarks)\n",
    "    return landmarks_list\n",
    "\n",
    "# Process all frames in the utterance\n",
    "def process_utterance_frames(frames):\n",
    "    for frame in frames:\n",
    "        landmarks = detect_landmarks(frame)\n",
    "        # landmarks now contain the (x, y) coordinates for facial landmarks\n",
    "        print(landmarks)  # For demonstration, you can store them as needed\n",
    "\n",
    "def extract_frames(video_path):\n",
    "    # Open video file\n",
    "    if not os.path.exists(video_path):\n",
    "        raise FileNotFoundError(f\"Video file not found: {video_path}\")\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "\n",
    "    start_frame = 0  # Define start_frame\n",
    "    end_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1  # Define end_frame as the last frame of the video\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)  # Set video to start frame\n",
    "\n",
    "    for _ in range(start_frame, end_frame + 1):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "\n",
    "    return frames\n",
    "\n",
    "def main():\n",
    "    interim_avi_list = [\n",
    "        f\"../data/interim/iemocap/session{i}/\" for i in range(1, 6)\n",
    "    ]\n",
    "\n",
    "    for i, video_dir in enumerate(interim_avi_list):\n",
    "        files = list_files_in_folder(video_dir)\n",
    "        for file in tqdm(files):\n",
    "            if file.split('.')[-1] != 'avi':\n",
    "                continue\n",
    "            video_path = video_dir + file\n",
    "            frames = extract_frames(video_path)\n",
    "            try:\n",
    "                process_utterance_frames(frames)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {e}\")\n",
    "                print(f\"Error occured at {video_path}, {lab_file}\")\n",
    "                break\n",
    "    # Usage\n",
    "    frames = [...]  # Assume frames are already extracted\n",
    "    process_utterance_frames(frames)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. High-Level Feature Extraction (Facial Action Units)\n",
    "\n",
    "    - Facial Action Unit (AU) Extraction:\n",
    "        - Use a pre-trained model like OpenFace or Py-Feat to extract Facial Action Units (AUs) for each frame. AUs correspond to specific facial muscle movements and represent high-level features.\n",
    "        Extract AU intensity values, which can help interpret emotions and map them to higher-level categories like \"angry\" or \"happy.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openface\n",
    "\n",
    "# Initialize OpenFace feature extractor\n",
    "face_model = openface.Face()\n",
    "\n",
    "def extract_action_units(image):\n",
    "    rgb_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    faus = face_model.detectFace(rgb_img, with_landmarks=True)  # Detect FAUs\n",
    "    return faus  # Returns FAU intensities and presence\n",
    "\n",
    "# Process frames to extract FAUs\n",
    "def extract_faus_for_frames(frames):\n",
    "    faus_per_frame = []\n",
    "    for frame in frames:\n",
    "        faus = extract_action_units(frame)\n",
    "        faus_per_frame.append(faus)\n",
    "    return faus_per_frame\n",
    "\n",
    "# Usage\n",
    "frames = [...]  # Assume frames are already extracted\n",
    "faus_per_frame = extract_faus_for_frames(frames)\n",
    "print(faus_per_frame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Low-Level Feature Extraction (Deep Learning Features)\n",
    "\n",
    "    - Low-Level Feature Extraction:\n",
    "        - Use deep learning-based models like CNNs (Convolutional Neural Networks) or ResNet to extract low-level pixel-based features directly from the raw face images.\n",
    "        These features capture textures, gradients, and other fine-grained details useful for deep learning-based emotion recognition models.\n",
    "\n",
    "    - Tools: Pre-trained CNNs (ResNet, VGG), custom CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load the pretrained VGG16 model from PyTorch\n",
    "model = models.vgg16(pretrained=True)\n",
    "model.classifier = nn.Identity()  # Remove the classification layer to get features\n",
    "\n",
    "# Define image transformation\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def extract_features(image):\n",
    "    image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))  # Convert to PIL image\n",
    "    image = preprocess(image)\n",
    "    image = image.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        features = model(image)\n",
    "    return features.numpy().flatten()\n",
    "\n",
    "# Process frames to extract low-level features\n",
    "def extract_cnn_features_for_frames(frames):\n",
    "    features_per_frame = []\n",
    "    for frame in frames:\n",
    "        features = extract_features(frame)\n",
    "        features_per_frame.append(features)\n",
    "    return features_per_frame\n",
    "\n",
    "# Usage\n",
    "frames = [...]  # Assume frames are already extracted\n",
    "low_level_features = extract_cnn_features_for_frames(frames)\n",
    "print(low_level_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Synchronization with Time Labels\n",
    "\n",
    "    - Map Features to Time Labels:\n",
    "        - For each extracted feature (both AU and CNN features), map them back to the time intervals provided in the dataset’s utterance summaries.\n",
    "        - This ensures that the features correspond to the correct utterance or time segment within the video.\n",
    "\n",
    "    - Tools: Pandas, Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emotion_labels(evaluation_file):\n",
    "    emotion_labels = {}\n",
    "    with open(evaluation_file, 'r') as f:\n",
    "        for line in f:\n",
    "            start_time, end_time, utterance_name, ground_truth, *dimensional = line.split()\n",
    "            emotion_labels[utterance_name] = {\n",
    "                'categorical': ground_truth,\n",
    "                'valence': dimensional[0],\n",
    "                'activation': dimensional[1],\n",
    "                'dominance': dimensional[2]\n",
    "            }\n",
    "    return emotion_labels\n",
    "\n",
    "# Usage\n",
    "evaluation_file = 'SessionX/dialog/Evaluation/Ses01F_impro01_eval.txt'\n",
    "emotion_labels = load_emotion_labels(evaluation_file)\n",
    "print(emotion_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Aggregation and Representation\n",
    "\n",
    "    - Aggregate Features:\n",
    "        - For high-level features (AUs), you can either average the AU intensity values over each time interval or select key frames.\n",
    "        For low-level features (CNN-extracted), consider methods like temporal pooling or sequence models (LSTM, GRU) to capture temporal dynamics.\n",
    "\n",
    "    - Tools: Scikit-learn, TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def normalize_features(features):\n",
    "    scaler = StandardScaler()\n",
    "    return scaler.fit_transform(features)\n",
    "\n",
    "def aggregate_features_per_utterance(features_per_frame):\n",
    "    # Example: Taking the mean of features across all frames for each utterance\n",
    "    return np.mean(features_per_frame, axis=0)\n",
    "\n",
    "# Usage\n",
    "features_per_frame = [...]  # Low-level or FAU features per frame\n",
    "normalized_features = normalize_features(features_per_frame)\n",
    "utterance_features = aggregate_features_per_utterance(normalized_features)\n",
    "print(utterance_features)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load features and labels\n",
    "X = [...]  # Features (low-level and/or FAUs)\n",
    "y = [...]  # Emotion labels (categorical or dimensional)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a classifier (example: RandomForest)\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "7. Output and Storage\n",
    "\n",
    "    Save Features:\n",
    "        Store both high-level (AUs) and low-level (CNN) features for each time segment in a structured format (e.g., CSV, HDF5). Ensure that these features are labeled with the corresponding utterance or time interval for easy retrieval.\n",
    "\n",
    "    Tools: Pandas, HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "sessions = [...]\n",
    "# Create or open an HDF5 file\n",
    "with h5py.File('iemocap_features.h5', 'w') as hdf:\n",
    "    for session_idx, session_data in enumerate(sessions):\n",
    "        # Create a group for each session\n",
    "        session_grp = hdf.create_group(f'session_{session_idx+1}')\n",
    "        for utterance_idx, utterance_data in enumerate(session_data):\n",
    "            # Create a subgroup for each utterance\n",
    "            utterance_grp = session_grp.create_group(f'utterance_{utterance_idx+1}')\n",
    "\n",
    "            # Store high-level (AU) features\n",
    "            au_data = np.array(utterance_data['high_level_features'])\n",
    "            utterance_grp.create_dataset('high_level_features', data=au_data, compression=\"gzip\")\n",
    "\n",
    "            # Store low-level (CNN) features\n",
    "            cnn_data = np.array(utterance_data['low_level_features'])\n",
    "            utterance_grp.create_dataset('low_level_features', data=cnn_data, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive\n",
    "\n",
    "Currently, code that is not in use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature extraction code\n",
    "### Currently, this code is just a placeholder and does not actually extract features\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load MELD CSV files\n",
    "train_df = pd.read_csv('train_sent_emo.csv')\n",
    "dev_df = pd.read_csv('dev_sent_emo.csv')\n",
    "test_df = pd.read_csv('test_sent_emo.csv')\n",
    "\n",
    "# Video processing function\n",
    "def extract_frames_from_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "# Audio processing function\n",
    "def extract_audio_features(audio_path):\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "    # Extract features here (e.g., MFCC)\n",
    "    # Placeholder for feature extraction code\n",
    "    return np.array([])  # Replace with actual feature extraction\n",
    "\n",
    "# Text processing function\n",
    "def preprocess_text(text):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    return tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "def main():\n",
    "    # Example usage\n",
    "    example_video_path = 'example.mp4'\n",
    "    frames = extract_frames_from_video(example_video_path)\n",
    "    audio_features = extract_audio_features('example.wav')\n",
    "    text_features = preprocess_text(\"example utterance text\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
